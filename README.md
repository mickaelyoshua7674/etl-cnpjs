# Documenta√ß√£o
## Descri√ß√£o
Este projeto tem como objetivo criar um Banco de Dados com todas as informa√ß√µes dispon√≠veis de CNPJs disponibilizados pela Receita Federal em https://dadosabertos.rfb.gov.br/CNPJ/ e serviu tamb√©m para meus estudos de Concorr√™ncia e Paralelismo (programa√ß√£o Ass√≠ncrona). Foram utilizadas tr√™s formas diferentes de programa√ß√£o Ass√≠ncrona: I/O, Processos e Threads.

Como todo projeto sempre possuir√° melhorias, este n√£o √© diferente. Aceito quaisquer sugest√µes, corre√ß√µes, conselhos e coment√°rios üòÅ.

## Ferramentas
* Sistema operacional: Windows 11;
* Liguagens de programa√ß√£o: Python-3.11.5 e SQL;
* M√≥dulos de Python: [requeriments.txt](requirements.txt);
* Banco de Dados: PostgreSQL;
* Ferramenta para acesso ao Banco de Dados: pgAdmin 4.

## Vis√£o geral do algoritmo do projeto
### Download dos arquivos
Download √© realizado utilizando l√≥gica Asynchronous I/O atrav√©s das bibliotecas [aiohttp](https://docs.aiohttp.org/en/stable/) e [asyncio](https://docs.python.org/3/library/asyncio.html). Atrav√©s de Web Scraping (usando [selenium](https://selenium-python.readthedocs.io/)) s√£o coletados os nomes dos arquivos no site, em seguida, de forma paralela, √© iniciado o download dos Bytes iterando chunks do conte√∫do do download (Stream), em seguida os Bytes s√£o descompactados e √© salvo o arquivo `.csv` resultante.

O script [download_unzip.py](download_unzip.py) √© o respons√°vel por fazer o download de todos os arquivos `.csv` necess√°rios.

### Transforma√ß√£o e Carregamento dos arquivos
Para a leitura e transforma√ß√£o dos arquivos foi utlizado o [pandas](https://pandas.pydata.org/docs/) e para conex√£o com o Banco de Dados e inser√ß√£o dos dados foi utilizado o [SQLAlchemy](https://docs.sqlalchemy.org/en/20/).

#### Arquivos menores
Os arquivos menores s√£o as tabelas das chaves estrangeiras (Foreign Keys) dos arquivos maiores, por serem pequenas quantidades de dados e tabelas de r√°pida cria√ß√£o e inser√ß√£o, foi totalmente feito de forma s√≠ncrona levando poucos segundos para ser executado.

O script [transform_and_load_small_tables.py](transform_and_load_small_tables.py) cria as tabelas e faz a povoa√ß√£o delas (inser√ß√£o dos dados).

#### Arquivos maiores
As tabelas maiores s√£o transformadas e inseridas atrav√©s da Stream dos arquivos com a seguinte estrutura em cada processo criado:

![schema process](unit_of_process.png)

Cada processo ir√° ler em `Stream` seu arquivo individual, cada chunk de arquivo lido ser√° processado com o `Pandas` e inserido numa `Queue` que √© compartilhada entre todas as `Threads` naquele processo. Cada Thread ir√° estabelecer uma conex√£o com o banco de dados e iterar na Queue inserindo os dados.

O processo de transformar e inserir dados se repete para cada chunk de dados.

Os processos criados possuem essa estrutura na imagem acima, sendo cada um lendo de um arquivo diferente, ou seja, n√£o h√° compartilhamento de dados entre processos, mas h√° compartilhamento entre Threads no mesmo processo atrav√©s da Queue.

O script [transform_and_load_big_tables.py](transform_and_load_big_tables.py) √© respons√°vel pela cria√ß√£o e povoa√ß√£o das tabelas maiores.

### OOP
A Programa√ß√£o Orientada a Objeto encontra-se na pasta [models](models) e √© utilizada na Transforma√ß√£o e Inser√ß√£o das tabelas maiores (na das menores √© utilizada apenas para pegar o `engine`).

As duas principais classes s√£o [BaseModel.py](models\BaseModel.py) e [MyThread.py](models\MyThread.py).

* [BaseModel.py](models\BaseModel.py) declara todas as vari√°veis e m√©todos necess√°rios para as classes de cada tabela maior, onde a √∫nica diferen√ßa entre as classes das tabelas s√£o o `schema`, `table_name`, `fk` (Foreign Keys) e o m√©todo `process_chunk`.
* [MyThread.py](models\MyThread.py) herda da classe `threading.Thread` reescrevendo o m√©todo `run()` para pegar dados da `Queue` e inserir no Banco de Dados.

## Configura√ß√£o do Ambiente
### Banco de Dados
Criar o Banco de Dados manualmente e localmente utilizando o pgAdmin 4 com as configura√ß√µes padr√£o.

### Python
Instalar vers√£o 3.11 ou mais recente de Python, em seguida criar um ambiente virtual executando o comando `python -m venv <nome da pasta>` no terminal (estando dentro da pasta do seu projeto) e ativar o abiente virtual com o comando `<nome da pasta>\Scripts\activate`.

Criar um arquivo `.env` para definir as vari√°veis de ambiente de acordo com o exemplo [.env_example](.env_example).

    Aten√ß√£o ‚ö†: Nas vari√°veis de ambiente THREADS_NUMBER e CHUNKSIZE a quantidade informada ser√° para cada processo definido, e.g.: com THREADS_NUMBER="8" voc√™ ter√° 8 Threads rodando em cada processo, com CHUNKSIZE="100000" ser√£o chunks de dados de tamanho 100000 sendo processados e inseridos no Bando de Dados para cada processo.

Finalmente executar o script [setup.py](setup.py) que ir√° instalar todas as depend√™ncias (m√≥dulos) de Python e definir as vari√°veis de ambiente infromadas no arquivo `.env`.

## Ordem de execu√ß√£o dos scripts
1. [setup.py](setup.py) -> Configura√ß√£o do ambiente (S√≠ncrono);
2. [download_unzip.py](download_unzip.py) -> Download dos arquivos (Ass√≠ncrono);
3. [transform_and_load_small_tables.py](transform_and_load_small_tables.py) -> Cria√ß√£o e povoa√ß√£o de tabelas menores (S√≠ncrono);
4. [transform_and_load_big_tables.py](transform_and_load_big_tables.py) -> Cria√ß√£o e povoa√ß√£o de tabelas maiores (Ass√≠ncrono).

## Poss√≠veis melhorias
* SQLAlchemy: N√£o fui muito a fundo na `engine` e conex√µes do SQLAlchemy, talvez tenha uma melhor performance caso seja utilizada [`Session()`](https://docs.sqlalchemy.org/en/20/orm/session.html) e at√© mesmo uso de [`SQLALchemy.orm`](https://docs.sqlalchemy.org/en/20/orm/) e sua cria√ß√£o de modelos de tabelas. Outra ideia seria mexer nas configura√ß√£o do `engine` de conex√£o;
* Testes: Este projeto n√£o possui testes automatizados, pretendo criar em algum momento utilizando o [`pytest`](https://docs.pytest.org/en/7.1.x/contents.html);
* Compiladores: Para tentar aumentar a performance, pricipalmente de processamento dos dados, utilizar compiladores como PyPy e estrat√©gias de Cython podem acrescentar um grande ganho. Ja estudei sobre por√©m nunca apliquei, seria um bom teste;
* Iser√ß√£o dos dados: Como o processo de inser√ß√£o em Banco de Dados Relacional n√£o √© algo exatamente r√°pido por causa da manuten√ß√£o das propriedades ACID (atomicity, consistency, isolation, and durability), n√£o sei dizer o que poderia ser feito para essa melhora (pricipalmente porque n√£o pesquisei sobre o assunto). Aceito sugest√µes üôÉ;
* Tabelas: criar chaves prim√°rias para as tabelas maiores;
* Stop point: m√©todos de rastrear at√© onde foram inseridos os dados de arquivos maiores e retomar a inser√ß√£o a partir desse ponto.
